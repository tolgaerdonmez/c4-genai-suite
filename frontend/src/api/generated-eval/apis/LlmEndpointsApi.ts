//@ts-nocheck
/* tslint:disable */
/* eslint-disable */
/**
 * LLM Eval Service
 * Internal evaluation service for C4 GenAI Suite
 *
 * The version of the OpenAPI document: 1.0.0
 *
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

import * as runtime from '../runtime';
import type {
  GenericError,
  HTTPValidationError,
  LLMEndpoint,
  LLMEndpointCreate,
  LLMEndpointDelete,
  LLMEndpointTypesResponse,
  LLMEndpointUpdate,
  PluginFeature,
} from '../models/index';
import {
  GenericErrorFromJSON,
  GenericErrorToJSON,
  HTTPValidationErrorFromJSON,
  HTTPValidationErrorToJSON,
  LLMEndpointFromJSON,
  LLMEndpointToJSON,
  LLMEndpointCreateFromJSON,
  LLMEndpointCreateToJSON,
  LLMEndpointDeleteFromJSON,
  LLMEndpointDeleteToJSON,
  LLMEndpointTypesResponseFromJSON,
  LLMEndpointTypesResponseToJSON,
  LLMEndpointUpdateFromJSON,
  LLMEndpointUpdateToJSON,
  PluginFeatureFromJSON,
  PluginFeatureToJSON,
} from '../models/index';

export interface LlmEndpointsDeleteRequest {
  llmEndpointId: string;
  lLMEndpointDelete: LLMEndpointDelete;
}

export interface LlmEndpointsGetRequest {
  llmEndpointId: string;
}

export interface LlmEndpointsGetAllRequest {
  q?: string | null;
  supportedFeatures?: Array<PluginFeature> | null;
  offset?: number;
  limit?: number;
}

export interface LlmEndpointsPatchRequest {
  llmEndpointId: string;
  lLMEndpointUpdate: LLMEndpointUpdate;
  xUserId?: string | null;
  xUserName?: string | null;
  xUserEmail?: string | null;
}

export interface LlmEndpointsPostRequest {
  lLMEndpointCreate: LLMEndpointCreate;
  xUserId?: string | null;
  xUserName?: string | null;
  xUserEmail?: string | null;
}

/**
 *
 */
export class LlmEndpointsApi extends runtime.BaseAPI {
  /**
   * Delete
   */
  async llmEndpointsDeleteRaw(
    requestParameters: LlmEndpointsDeleteRequest,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<runtime.ApiResponse<any>> {
    if (requestParameters['llmEndpointId'] == null) {
      throw new runtime.RequiredError(
        'llmEndpointId',
        'Required parameter "llmEndpointId" was null or undefined when calling llmEndpointsDelete().',
      );
    }

    if (requestParameters['lLMEndpointDelete'] == null) {
      throw new runtime.RequiredError(
        'lLMEndpointDelete',
        'Required parameter "lLMEndpointDelete" was null or undefined when calling llmEndpointsDelete().',
      );
    }

    const queryParameters: any = {};

    const headerParameters: runtime.HTTPHeaders = {};

    headerParameters['Content-Type'] = 'application/json';

    const response = await this.request(
      {
        path: `/v1/llm-endpoints/{llm_endpoint_id}`.replace(
          `{${'llm_endpoint_id'}}`,
          encodeURIComponent(String(requestParameters['llmEndpointId'])),
        ),
        method: 'DELETE',
        headers: headerParameters,
        query: queryParameters,
        body: LLMEndpointDeleteToJSON(requestParameters['lLMEndpointDelete']),
      },
      initOverrides,
    );

    if (this.isJsonMime(response.headers.get('content-type'))) {
      return new runtime.JSONApiResponse<any>(response);
    } else {
      return new runtime.TextApiResponse(response) as any;
    }
  }

  /**
   * Delete
   */
  async llmEndpointsDelete(
    llmEndpointId: string,
    lLMEndpointDelete: LLMEndpointDelete,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<any> {
    const response = await this.llmEndpointsDeleteRaw(
      { llmEndpointId: llmEndpointId, lLMEndpointDelete: lLMEndpointDelete },
      initOverrides,
    );
    return await response.value();
  }

  /**
   * Get
   */
  async llmEndpointsGetRaw(
    requestParameters: LlmEndpointsGetRequest,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<runtime.ApiResponse<LLMEndpoint>> {
    if (requestParameters['llmEndpointId'] == null) {
      throw new runtime.RequiredError(
        'llmEndpointId',
        'Required parameter "llmEndpointId" was null or undefined when calling llmEndpointsGet().',
      );
    }

    const queryParameters: any = {};

    const headerParameters: runtime.HTTPHeaders = {};

    const response = await this.request(
      {
        path: `/v1/llm-endpoints/{llm_endpoint_id}`.replace(
          `{${'llm_endpoint_id'}}`,
          encodeURIComponent(String(requestParameters['llmEndpointId'])),
        ),
        method: 'GET',
        headers: headerParameters,
        query: queryParameters,
      },
      initOverrides,
    );

    return new runtime.JSONApiResponse(response, (jsonValue) => LLMEndpointFromJSON(jsonValue));
  }

  /**
   * Get
   */
  async llmEndpointsGet(llmEndpointId: string, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<LLMEndpoint> {
    const response = await this.llmEndpointsGetRaw({ llmEndpointId: llmEndpointId }, initOverrides);
    return await response.value();
  }

  /**
   * Get All
   */
  async llmEndpointsGetAllRaw(
    requestParameters: LlmEndpointsGetAllRequest,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<runtime.ApiResponse<Array<LLMEndpoint>>> {
    const queryParameters: any = {};

    if (requestParameters['q'] != null) {
      queryParameters['q'] = requestParameters['q'];
    }

    if (requestParameters['supportedFeatures'] != null) {
      queryParameters['supported_features'] = requestParameters['supportedFeatures'];
    }

    if (requestParameters['offset'] != null) {
      queryParameters['offset'] = requestParameters['offset'];
    }

    if (requestParameters['limit'] != null) {
      queryParameters['limit'] = requestParameters['limit'];
    }

    const headerParameters: runtime.HTTPHeaders = {};

    const response = await this.request(
      {
        path: `/v1/llm-endpoints`,
        method: 'GET',
        headers: headerParameters,
        query: queryParameters,
      },
      initOverrides,
    );

    return new runtime.JSONApiResponse(response, (jsonValue) => jsonValue.map(LLMEndpointFromJSON));
  }

  /**
   * Get All
   */
  async llmEndpointsGetAll(
    q?: string | null,
    supportedFeatures?: Array<PluginFeature> | null,
    offset?: number,
    limit?: number,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<Array<LLMEndpoint>> {
    const response = await this.llmEndpointsGetAllRaw(
      { q: q, supportedFeatures: supportedFeatures, offset: offset, limit: limit },
      initOverrides,
    );
    return await response.value();
  }

  /**
   * Get Types
   */
  async llmEndpointsGetTypesRaw(
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<runtime.ApiResponse<LLMEndpointTypesResponse>> {
    const queryParameters: any = {};

    const headerParameters: runtime.HTTPHeaders = {};

    const response = await this.request(
      {
        path: `/v1/llm-endpoints/types`,
        method: 'GET',
        headers: headerParameters,
        query: queryParameters,
      },
      initOverrides,
    );

    return new runtime.JSONApiResponse(response, (jsonValue) => LLMEndpointTypesResponseFromJSON(jsonValue));
  }

  /**
   * Get Types
   */
  async llmEndpointsGetTypes(initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<LLMEndpointTypesResponse> {
    const response = await this.llmEndpointsGetTypesRaw(initOverrides);
    return await response.value();
  }

  /**
   * Patch
   */
  async llmEndpointsPatchRaw(
    requestParameters: LlmEndpointsPatchRequest,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<runtime.ApiResponse<LLMEndpoint>> {
    if (requestParameters['llmEndpointId'] == null) {
      throw new runtime.RequiredError(
        'llmEndpointId',
        'Required parameter "llmEndpointId" was null or undefined when calling llmEndpointsPatch().',
      );
    }

    if (requestParameters['lLMEndpointUpdate'] == null) {
      throw new runtime.RequiredError(
        'lLMEndpointUpdate',
        'Required parameter "lLMEndpointUpdate" was null or undefined when calling llmEndpointsPatch().',
      );
    }

    const queryParameters: any = {};

    const headerParameters: runtime.HTTPHeaders = {};

    headerParameters['Content-Type'] = 'application/json';

    if (requestParameters['xUserId'] != null) {
      headerParameters['x-user-id'] = String(requestParameters['xUserId']);
    }

    if (requestParameters['xUserName'] != null) {
      headerParameters['x-user-name'] = String(requestParameters['xUserName']);
    }

    if (requestParameters['xUserEmail'] != null) {
      headerParameters['x-user-email'] = String(requestParameters['xUserEmail']);
    }

    const response = await this.request(
      {
        path: `/v1/llm-endpoints/{llm_endpoint_id}`.replace(
          `{${'llm_endpoint_id'}}`,
          encodeURIComponent(String(requestParameters['llmEndpointId'])),
        ),
        method: 'PATCH',
        headers: headerParameters,
        query: queryParameters,
        body: LLMEndpointUpdateToJSON(requestParameters['lLMEndpointUpdate']),
      },
      initOverrides,
    );

    return new runtime.JSONApiResponse(response, (jsonValue) => LLMEndpointFromJSON(jsonValue));
  }

  /**
   * Patch
   */
  async llmEndpointsPatch(
    llmEndpointId: string,
    lLMEndpointUpdate: LLMEndpointUpdate,
    xUserId?: string | null,
    xUserName?: string | null,
    xUserEmail?: string | null,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<LLMEndpoint> {
    const response = await this.llmEndpointsPatchRaw(
      {
        llmEndpointId: llmEndpointId,
        lLMEndpointUpdate: lLMEndpointUpdate,
        xUserId: xUserId,
        xUserName: xUserName,
        xUserEmail: xUserEmail,
      },
      initOverrides,
    );
    return await response.value();
  }

  /**
   * Post
   */
  async llmEndpointsPostRaw(
    requestParameters: LlmEndpointsPostRequest,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<runtime.ApiResponse<LLMEndpoint>> {
    if (requestParameters['lLMEndpointCreate'] == null) {
      throw new runtime.RequiredError(
        'lLMEndpointCreate',
        'Required parameter "lLMEndpointCreate" was null or undefined when calling llmEndpointsPost().',
      );
    }

    const queryParameters: any = {};

    const headerParameters: runtime.HTTPHeaders = {};

    headerParameters['Content-Type'] = 'application/json';

    if (requestParameters['xUserId'] != null) {
      headerParameters['x-user-id'] = String(requestParameters['xUserId']);
    }

    if (requestParameters['xUserName'] != null) {
      headerParameters['x-user-name'] = String(requestParameters['xUserName']);
    }

    if (requestParameters['xUserEmail'] != null) {
      headerParameters['x-user-email'] = String(requestParameters['xUserEmail']);
    }

    const response = await this.request(
      {
        path: `/v1/llm-endpoints`,
        method: 'POST',
        headers: headerParameters,
        query: queryParameters,
        body: LLMEndpointCreateToJSON(requestParameters['lLMEndpointCreate']),
      },
      initOverrides,
    );

    return new runtime.JSONApiResponse(response, (jsonValue) => LLMEndpointFromJSON(jsonValue));
  }

  /**
   * Post
   */
  async llmEndpointsPost(
    lLMEndpointCreate: LLMEndpointCreate,
    xUserId?: string | null,
    xUserName?: string | null,
    xUserEmail?: string | null,
    initOverrides?: RequestInit | runtime.InitOverrideFunction,
  ): Promise<LLMEndpoint> {
    const response = await this.llmEndpointsPostRaw(
      { lLMEndpointCreate: lLMEndpointCreate, xUserId: xUserId, xUserName: xUserName, xUserEmail: xUserEmail },
      initOverrides,
    );
    return await response.value();
  }
}
